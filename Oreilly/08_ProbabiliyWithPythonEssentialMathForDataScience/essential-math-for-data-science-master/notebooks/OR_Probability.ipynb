{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "<!-- requirement: images/rv_flowchart.png -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def plot_hist_dist(rvs, dist, title=None, label='', ax=None, line_color='black', bar_color='blue'):\n",
    "    ax = ax if ax else plt.gca()\n",
    "    _, bins, _ = ax.hist(rvs, bins=50, alpha=.6, density=True, label=(label + ' rvs').strip(), color=bar_color)\n",
    "    xmin, xmax = bins.min(), bins.max()\n",
    "    xpoints = np.arange(xmin, xmax, (xmax - xmin) / 100)\n",
    "    ax.plot(xpoints, dist.pdf(xpoints), label=(label+' pdf').strip(), color=line_color)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.legend()\n",
    "    \n",
    "def plot_hist_dist_discrete(rvs, dist, title=None, label='', ax=None, line_color='black', bar_color='blue'):\n",
    "    ax = ax if ax else plt.gca()\n",
    "    uniques = np.unique(rvs)\n",
    "    mids = (uniques[1:] + uniques[:-1]) / 2.\n",
    "    bins = np.hstack([[uniques[0]-.5], mids, [uniques[-1] + .5]])\n",
    "    plt.hist(rvs, bins=bins, density=True, label=(label + ' rvs').strip(), alpha=.6, color=bar_color)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    plt.plot(uniques, dist.pmf(uniques), label=(label + ' pmf').strip(), color=line_color)\n",
    "    ax.legend()\n",
    "    \n",
    "def print_mean_variance(rvs, dist):\n",
    "    print(\"Mean:     Theoretical {:.4f}, Actual {:.4f}\".format(dist.mean(), rvs.mean()))\n",
    "    print(\"Variance: Theoretical {:.4f}, Actual {:.4f}\".format(dist.var(),  rvs.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "from functools import wraps\n",
    "\n",
    "def gen_interact_dist(plotter):\n",
    "    @wraps(plotter)\n",
    "    def wrapper(dist, xlim, ylim, N=2000):\n",
    "        def wrapped(**kwargs):\n",
    "            _dist = dist(**kwargs)\n",
    "            rvs = _dist.rvs(N)\n",
    "\n",
    "            plotter(_dist, rvs, xlim)\n",
    "\n",
    "            plt.title(_dist.dist.name)\n",
    "            plt.xlim(xlim)\n",
    "            plt.ylim(ylim)\n",
    "            plt.legend()\n",
    "        return wrapped\n",
    "    return wrapper\n",
    "\n",
    "@gen_interact_dist\n",
    "def interact_dist(dist, rvs, xlim):\n",
    "    \"\"\"\n",
    "    Function plots rvs and pdf given a dist, xlim, and ylim\n",
    "    \"\"\"\n",
    "    x = np.linspace(*xlim, num=5000)\n",
    "    p = dist.pdf(x)\n",
    "    plt.hist(rvs, bins=50, density=True, alpha=0.6, color='blue', label='rvs')\n",
    "    plt.plot(x, p, color='black', label='pdf')\n",
    "\n",
    "@gen_interact_dist\n",
    "def interact_dist_discrete(dist, rvs, xlim):\n",
    "    \"\"\"\n",
    "    Function plots rvs and pmf given a dist, xlim, and ylim\n",
    "    \"\"\"\n",
    "    x = np.linspace(*xlim, num=5000, dtype=int)\n",
    "    p = dist.pmf(x)\n",
    "            \n",
    "    uniques = np.unique(rvs)\n",
    "    mids = (uniques[1:] + uniques[:-1]) / 2.\n",
    "    bins = np.hstack([[uniques[0]-.5], mids, [uniques[-1] + .5]])\n",
    "    \n",
    "    plt.hist(rvs, bins=bins, density=True, alpha=0.6, color='blue', label='rvs')\n",
    "    plt.plot(x, p, color='black', label='pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability and statistics are based on the notion of a **random variable**.  Random variables are *abstract mathematical quantities* (often denoted $X$) which can take on values at random.  Random variates (`rvs`) are instances of random variables.\n",
    "\n",
    "**Example:** Suppose we roll a standard fair die.  The number shown on top is a random variable taking on the values 1 through 6, inclusive.\n",
    "\n",
    "While the value of a variate can be anything, we know what values tend to be based on a **distribution**, $p(x)$, which is the probability of $X$ taking on any value $x$.  Their distributions are how we reason about randomness mathematically.\n",
    "\n",
    "**Example:** In the previous dice example, the dice rolls are distributed evenly ($\\frac{1}{6}$ probability of each event) over all six possible values.\n",
    "\n",
    "While people (including us) often use a random variable and its distribution interchangeably, these are *not* the same concept and the distinction is not semantic.\n",
    "\n",
    "**Example:** Suppose we have two dice, one red and one blue but otherwise identical.  Their rolls would be represented by two different random variables.  These two random variables have the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_hist_dist(norm.rvs(size=2000), norm, title=norm.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics of Random Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For discrete-valued random variables, we can plot the distribution (or more precisely, the probability mass function or **PMF**, $p(x)$, which denotes the probability that $X = x$) of a random variable.  The **PMF** has the critical properties that it is non-negative $(p(x) \\ge 0)$ and \n",
    "$$ \\sum_x p(x) = 1 \\,.$$\n",
    "\n",
    "We can compute statistics about random variables using the expectation operator $\\mathbb{E}$.  The expectation operator has the following definition:\n",
    "$$ \\mathbb{E}[f(x)] = \\sum f(x) p(x) $$\n",
    "\n",
    "A simple (and important) example is the mean\n",
    "$$ \\mathbb{E}[X] = \\sum x p(x) $$\n",
    "\n",
    "We can also calculate the variance\n",
    "$$ \\mbox{Var}[X] = \\mathbb{E}\\left[(X-\\mathbb{E}[X])^2\\right] = \\sum x^2 p(x) - \\left( \\sum x p(x) \\right)^2 \\,. $$\n",
    "The standard deviation is simply the square root of the variance.\n",
    "\n",
    "We may also wish to calculate the probability $\\mathbb{P}$ of different **events** associated with these random variables.  For example the cumulative distribution function or **CDF** of a random variable is associated with the events\n",
    "\n",
    "$$ F_X(a) = \\mathbb{P}[X \\le a] = \\sum_{x \\le a} p(x)\\,. $$\n",
    "\n",
    "**Examples:** There are a number of important random variables.  The discrete ones have distributions such as:\n",
    "\n",
    "1. Bernoulli Distribution\n",
    "1. Binomial Distribution\n",
    "1. Geometric Distribution\n",
    "1. Poisson Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only given the formulas for discrete-valued random variables.  Continuous random variables have a probability distribution function or **PDF**, also denoted $p(x)$.  A PDF has to be non-negative ($p(x) \\ge 0$) and satisfy\n",
    "$$ \\int_{(-\\infty, \\infty)} p(x) dx = 1\\,. $$\n",
    "\n",
    "Similarly you will also hear about the cumulative distribution function or **CDF** defined as\n",
    "$$ F_X(x) = \\int_{(-\\infty, x]} p(y)dy\\,. $$\n",
    "\n",
    "The probability density of $X = x$ is given by $p(x)$.  Put another way, the probability that $a < X \\le b$ is given by the integral\n",
    "$$ \\mathbb{P}[a < X \\le b] = \\int_{(a, b]}p(x)dx = F_X(b) - F_X(a) \\,. $$\n",
    "\n",
    "We can extend the statistics of discrete random variables to continuous random variables by exchanging discrete sums for continuous integrals.\n",
    "\n",
    "$$ \\mathbb{E}[f(x)] = \\int_{(-\\infty, \\infty)} f(x) p(x) dx $$\n",
    "\n",
    "\n",
    "**Examples:** Continuous random variables may have distributions such as:\n",
    "\n",
    "1. Beta Distribution\n",
    "1. Exponential Distribution\n",
    "1. Normal Distribution\n",
    "\n",
    "**Exercise:** What would the corresponding formulas for mean and standard deviation look like for continuous-valued random variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some examples of distributions\n",
    "\n",
    "dists = (sp.stats.expon(),\n",
    "         sp.stats.norm(),\n",
    "         sp.stats.beta(a=5, b=10))\n",
    "for k, dist in enumerate(dists):\n",
    "    ax = plt.subplot(3,2,2*k+1)\n",
    "    plot_hist_dist(dist.rvs(size=2000), dist, title=dist.dist.name, ax=ax)\n",
    "\n",
    "    xs = np.linspace(*plt.xlim())\n",
    "    ax = plt.subplot(3,2,2*k+2)\n",
    "    ax.plot(xs, dist.cdf(xs), label='cdf')\n",
    "    mean, var = dist.stats('mv')\n",
    "    ax.plot([mean]*2, [0., 1.], label='mean')\n",
    "    plt.text(mean, 0.5, 'std', ha='center', va='bottom')\n",
    "    plt.annotate(\"\", xy=(mean-np.sqrt(var), 0.5), xycoords='data',\n",
    "                 xytext=(mean+np.sqrt(var), 0.5), textcoords='data',\n",
    "                 arrowprops=dict(arrowstyle=\"|-|\", lw=2, color='r'))    \n",
    "\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_title(dist.dist.name)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **quantile function** (denoted $q_X(u)$) is the inverse of the cumulative distribution function or CDF $F_X(x)$.  That is,\n",
    "\n",
    "$$ q_X (F_X(x)) = x \\qquad F_X(q_X(u)) = u$$\n",
    "\n",
    "Notice that since the range of $F_X$ is only $(0,1)$, then $q_X$ is a function on $(0,1)$.\n",
    "\n",
    "**Theorem:** If $U$ is a uniform random variable then $q_X(U)$ is has the same distribution as $X$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quantile functions to generate random variables\n",
    "\n",
    "N = 1000\n",
    "\n",
    "dists = (\n",
    "    sp.stats.expon(scale=1/2.),\n",
    "    sp.stats.beta(a=2., b=4.),\n",
    "    sp.stats.norm(loc=2., scale=4.),\n",
    "    sp.stats.chi2(df=4.)\n",
    ")\n",
    "\n",
    "rvs = sp.stats.uniform().rvs(size=N)\n",
    "for k, dist in enumerate(dists):\n",
    "    ax = plt.subplot(2,2,k+1)\n",
    "    plot_hist_dist(dist.ppf(rvs), dist, title=dist.dist.name, ax=ax)\n",
    "plt.suptitle('rvs from quantile functions')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling observations with random variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many phenomena in the world are modeled as random variables. By considering the characteristics of the phenomena we would like to model, we can assume an appropriate random variable. Such assumptions lay the groundwork for statistic inference about the world. We can use measurements of the observed phenomena to compute statistics and estimate parameters of our model.\n",
    "\n",
    "The following flow-chart is a short guide to some of the more common random variables we might use as models of real-world phenomena.\n",
    "\n",
    "![Random variable flowchart](images/rv_flowchart.png)\n",
    "\n",
    "Simply knowing whether your observed values will be discrete or continuous, bounded or unbounded, already puts constraints on what kinds of random variables we might use to model a given phenomenon. However, a random variable will often only be appropriate as a model if the phenomenon meets certain assumptions. We'll explore each of the above random variables in more depth and expand upon these assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli, binomial, and geometric random variables are all defined in terms of **Bernoulli trials**. A Bernoulli trial is an event that is either a success or a failure. For example, a coin flip results in heads or tails, and we might define heads as success and failure as tails.\n",
    "\n",
    "Bernoulli trials are assumed to be _independent_. If we are considering a random variable based on several Bernoulli trials (i.e. binomial or geometric random variables), we might also require that the outcomes of the Bernoulli trials are _identically distributed_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Random Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli random variable is the simplest random variable in probability.  The value of the trial $X$ can take on either $0$ (failure) or $1$ (success) and it is parameterized by the value $\\Pi$, which is the probability that it is a success.  The PMF is given by\n",
    "\n",
    "$$ p(x) = \\left\\{ \\begin{align} \\Pi && \\mbox{if }x=1 \\\\ 1-\\Pi && \\mbox{if }x = 0 \\end{align} \\right. \\,.$$\n",
    "\n",
    "**Examples:**\n",
    "- We can think of $p$ as the probability a (biased) coin lands on heads after a flip.\n",
    "- Whether a user clicks on a displayed advertisement.\n",
    "- Whether a borrower defaults on a loan.\n",
    "\n",
    "**Stats:**\n",
    "- The mean is $\\mathbb{E}[X] = \\Pi$\n",
    "- The variance is $\\mbox{Var}[X] = \\Pi(1-\\Pi)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mean and variance of the Bernoulli\n",
    "\n",
    "p = .4\n",
    "\n",
    "dist = sp.stats.bernoulli(p)\n",
    "rvs = dist.rvs(size=1000)\n",
    "print_mean_variance(rvs, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence and Conditional Expectation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two random variables are independent if knowing the results of one does not tell you anything about the other.  That is, if $p_{X, Y}(x,y)$ is the joint PMF or PDF for $(X,Y)$ and $p_X(x)$ and $p_Y(y)$ are the PMF or PDF for $X$ and $Y$, then a variable is independent if and only if\n",
    "\n",
    "$$ p_{X, Y}(x,y) = p_X(x)p_Y(y)\\,. $$\n",
    "\n",
    "The **covariance** of two random variables $X$ and $Y$ is given by\n",
    "\n",
    "$$ \\begin{split} \\mbox{Cov}[X,Y] &= \\mathbb{E}\\left[(X - \\mathbb{E}[X]) (Y - \\mathbb{E}[Y])\\right] \\\\\n",
    "&= \\mathbb{E}[XY] - \\mathbb{E}\\left[X \\,\\mathbb{E}[Y]\\right] - \\mathbb{E}\\left[Y \\,\\mathbb{E}[X]\\right] + \\mathbb{E}[X]\\mathbb{E}[Y] \\\\\n",
    "&= \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] \\end{split} $$\n",
    "\n",
    "If two variables are independent, their *covariance* is zero, or equivalently the expectation of the product is just the product of their expectations\n",
    "$$ \\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y] \\,.$$\n",
    "\n",
    "**Warning:** Just because two variables have zero covariance does not mean they are independent!  For example, if $X$ is a random variable taking on the values $\\{-1, 0, 1\\}$ with equal probability and $Y = X^2$, then they have zero covariance but are not independent.\n",
    "\n",
    "When speaking of the joint distribution of random variables, it is often useful to speak of the **conditional expectation** given by\n",
    "$$ \\mathbb{E}[X \\mid Y=y] = \\sum_{x} x \\frac{p_{X, Y}(x,y)}{p_Y(y)}\\,. $$\n",
    "It's useful to think of the above conditional expectation as a function of $y$.  We may also want to take the **conditional probability** of events related to $X$,\n",
    "$$ \\mathbb{P}[X \\le a \\mid Y=y] = \\sum_{x \\le a} \\frac{p_{X, Y}(x,y)}{p_Y(y)}\\,. $$\n",
    "\n",
    "**Theorem:** If you have two independent random variables, $X$ and $Y$, the means add\n",
    "$$ \\mathbb{E}[X + Y] = \\mathbb{E}[X] + \\mathbb{E}[Y] $$\n",
    "and variances add\n",
    "$$ \\mbox{Var}[X + Y] = \\mbox{Var}[X] + \\mbox{Var}[Y]\\,. $$\n",
    "\n",
    "**Exercise:** What is the standard deviation of $X + Y$?\n",
    "1. What is the standard deviation if you add $n$ independent and identically distributed random variables:\n",
    "$$X_1 + \\cdots + X_n$$\n",
    "2. What is the standard deviation of the average:\n",
    "$$ \\frac{X_1 + \\cdots + X_n}{n} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Independent and dependent Bernoulli random variables\n",
    "\n",
    "p = .6\n",
    "q = .9\n",
    "r = p / (1-p) * (1-q)\n",
    "N = 2500\n",
    "\n",
    "# independent random variables\n",
    "rvs_x1 = sp.stats.bernoulli(p).rvs(size=N)\n",
    "rvs_y1 = sp.stats.bernoulli(p).rvs(size=N)\n",
    "\n",
    "# dependent random variables\n",
    "rvs_x2 = sp.stats.bernoulli(p).rvs(size=N)\n",
    "rvs_y2 = sp.stats.bernoulli(np.where(rvs_x2, q, r)).rvs(size=N)\n",
    "\n",
    "cov1 = np.cov(rvs_x1, rvs_y1)\n",
    "cov2 = np.cov(rvs_x2, rvs_y2)\n",
    "\n",
    "vmin = min(cov1.min(), cov2.min())\n",
    "vmax = min(cov1.max(), cov2.max())\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.matshow(cov1, fignum=False, vmin=vmin, vmax=vmax, cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('independent cov', y=1.15)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.matshow(cov2, fignum=False, vmin=vmin, vmax=vmax, cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('dependent cov', y=1.15)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "pd.Series({\n",
    "    'X=0': rvs_y1[rvs_x1==0].mean(),\n",
    "    'X=1': rvs_y1[rvs_x1==1].mean(),\n",
    "    'X=*': rvs_y1.mean(),\n",
    "}).plot(kind='bar', title='conditional mean (independent)')\n",
    "plt.ylim([0., 1.])\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "pd.Series({\n",
    "    'X=0': rvs_y2[rvs_x2==0].mean(),\n",
    "    'X=1': rvs_y2[rvs_x2==1].mean(),\n",
    "    'X=*': rvs_y2.mean(),\n",
    "}).plot(kind='bar', title='conditional mean (dependent)')\n",
    "plt.ylim([0., 1.])\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Random Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Binomial random variable is just the sum of $n$ independent Bernoulli random trials each with an independent probability $\\Pi$ of success.  Its PMF is given by the binomial theorem:\n",
    "\n",
    "$$ p(k) = \\binom{n}{k} \\Pi^k(1-\\Pi)^{n-k}\\,. $$\n",
    "\n",
    "You can easily derive this formula by considering a coin with a probability, $\\Pi$, of flipping heads and calculating the probability of seeing $k$ heads out of $n$ flips.\n",
    "\n",
    "**Examples:**\n",
    "- The total number of times a biased coin lands on heads after being flipped $n$ times.\n",
    "- The total number of ad clicks after seeing $n$ ads if a user has an independent probability $\\Pi$ of clicking on an advertisement.\n",
    "- The number of points scored after $n$ free-throw attempts if each attempt has $\\Pi$ probability of succeeding.\n",
    "\n",
    "**Stats:**\n",
    "- The mean is $\\mathbb{E}[X] = n\\Pi$\n",
    "- The variance is $\\mbox{Var}[X] = n\\Pi(1-\\Pi)$.\n",
    "\n",
    "Notice that the mean increases with both $n$ and $\\Pi$ but that the variance decreases with $\\Pi$ for a fixed mean: $\\mbox{Var}[X] = \\mathbb{E}[X] \\cdot (1-\\Pi)$.  This is illustrated in the plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binomial distribution, two parameters\n",
    "\n",
    "n_slider = IntSlider(value=10, min=5, max=50, step=1, description='n')\n",
    "pi_slider = FloatSlider(value=0.5, min=0.2, max=0.8, step=0.05, description='$\\Pi$')\n",
    "interact(\n",
    "    interact_dist_discrete(sp.stats.binom, xlim=[0, 18], ylim=[0, 0.3]), \n",
    "    n=n_slider, p=pi_slider\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem:** The sum of $n$ independent Bernoulli trials with probability $\\Pi$ of success is a Binomial with parameters $n$ and $\\Pi$.\n",
    "\n",
    "**Exercise:** Prove the formulas for mean and variance using the fact that the Binomial is the sum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binomial is the sum of Bernoulli\n",
    "\n",
    "p = .4\n",
    "n = 6\n",
    "\n",
    "rvs = sp.stats.bernoulli(p).rvs(size=[n, 1000]).sum(axis=0)\n",
    "dist=sp.stats.binom(n, p)\n",
    "\n",
    "plot_hist_dist_discrete(rvs, dist, title=\"Binomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geometric distribution models the number $X$ of independent Bernoulli trials needed to obtain a success -- that is, we keep testing independent Bernoulli random variables until we are successful and measure the (random) number of such trials.  The PMF is given by\n",
    "\n",
    "$$ p(k) = (1-\\Pi)^{k-1}\\Pi \\,. $$\n",
    "\n",
    "**Examples:**\n",
    "- Suppose a chef needs to crack an egg without breaking a yolk.  If she keeps the yolks intact with probability $p$,  then $p(k)$ is the PMF over the number of eggs it takes.\n",
    "- A newly-wed couple plans to have children until they have a girl.  The total number of children they expect to have is geometrically distributed.\n",
    "- A person being treated for cancer is given chemotherapy drugs which have independent and identically distributed probability $\\Pi$ of successful treatment.  The number of drugs he must take until he is successfully treated is geometrically distributed.\n",
    "\n",
    "**Stats:**\n",
    "- The mean is $\\mathbb{E}[X] = \\frac{1}{\\Pi}$\n",
    "- The variance is $\\mbox{Var}[X] = \\frac{1-\\Pi}{\\Pi^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The geometric distribution is the number of Bernoulli trials\n",
    "# before a successful trial\n",
    "\n",
    "p = .8\n",
    "\n",
    "def geom(p):\n",
    "    dist = sp.stats.bernoulli(p)\n",
    "    k = 1\n",
    "    while not dist.rvs():\n",
    "        k += 1\n",
    "    return k\n",
    "\n",
    "rvs = np.zeros(shape=100)\n",
    "for k in range(len(rvs)):\n",
    "    rvs[k] = geom(p)\n",
    "\n",
    "dist = sp.stats.geom(p)\n",
    "plot_hist_dist_discrete(rvs, dist, title=\"Geometric Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\Pi$, the probability of success for any given trial, increases, we would expect the first success to occur early and the mean of the geometric random variable to be lower.  This is born out in the plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Geometric distribution, one parameter\n",
    "\n",
    "Pi_slider = FloatSlider(value=0.5, min=0.05, max=1.0, step=0.05, description='$\\Pi$')\n",
    "interact(\n",
    "    interact_dist_discrete(sp.stats.geom, xlim=[1, 20], ylim=[0, 0.8]),\n",
    "    p=Pi_slider\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memoryless Property\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geometric distribution is _memoryless_.  Suppose your first successful Bernoulli trial is distributed $p(k)$.  Because the draws of the Bernoulli trials are independent, if you have $n$ failed trials, the distribution of *remaining* number of trials until success is still $p(k)$.  The process forgets its history. Past failures do not influence future probability of success.\n",
    "\n",
    "**Examples:** After breaking $n$ eggs, the distribution of the number of eggs our chef must still crack is the same as when she started.\n",
    "\n",
    "Mathematically, we observe that if $X$ is distributed as $p(k)$,\n",
    "$$ P[X=(n+k) \\mid X \\ge n] = (1-\\Pi)^{k-1}\\Pi = P[X=k] $$\n",
    "\n",
    "**Exercise:** Use the definition of conditional probability to prove the above statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The memoryless property of the geometric distribution\n",
    "\n",
    "p = .4\n",
    "n = 5\n",
    "N = 2000\n",
    "\n",
    "dist = sp.stats.geom(p)\n",
    "rvs = dist.rvs(size=N)\n",
    "plot_hist_dist_discrete(rvs[rvs > n] - n, dist, title=\"Conditional Geometric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Poisson distribution is a discrete-valued distribution representing the counts of independent events occurring within a fixed interval of time or space (which could be abstract, i.e not necessarily physical space).  The Poisson distribution describes an _arrival process_ (sometimes called a Poisson process), in which events occur at random with some fixed average rate. The Poisson distribution makes a few key assumptions:\n",
    "- Independence: the occurrence of any single event does not affect the likelihood of another\n",
    "- The events have the same likelihood of occurring on average\n",
    "- The probability of an event occurring is proportional to the length of the interval\n",
    "\n",
    "The distribution is parameterized by the rate parameter, $\\lambda$, and has a PMF\n",
    "\n",
    "$$ p(k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} $$\n",
    "\n",
    "**Stats:**\n",
    "- The mean is $\\mathbb{E}[X] = \\lambda$\n",
    "- The variance is $\\mbox{Var}[X] = \\lambda$.\n",
    "\n",
    "**Examples:**\n",
    "- The number of goals scored in a World Cup match.\n",
    "- The number of patient arrivals to an emergency room in one day.\n",
    "- The number of asteroids hitting the earth each year.\n",
    "- The number of Prussian Cavalry Officers kicked by their horse each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Poisson distribution, one parameter\n",
    "\n",
    "lambda_slider = FloatSlider(value=2.0, min=2.0, max=8.0, step=0.05, description='$\\lambda$')\n",
    "interact(\n",
    "    interact_dist_discrete(sp.stats.poisson, xlim=[0, 20], ylim=[0, 0.28]),\n",
    "    mu=lambda_slider\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponential distribution is a non-negative continuous random variable whose PDF is shaped exponentially\n",
    "\n",
    "$$ p(x) = \\lambda e^{-\\lambda x} $$\n",
    "\n",
    "The length of time (or space) between events in an arrival process are exponentially distributed. Therefore, the exponential distribution is closely related to the Poisson distribution, and they share the parameter, $\\lambda$.\n",
    "\n",
    "For example, imagine we track the arrival of customers at a store during a business day, and assume the customers arrive randomly with some average rate. When the store opens, we start a stopwatch, and when the first customer arrives we record the time on the stopwatch. We then reset the stopwatch and measure the length of time between the arrival of the first customer and the arrival of the second customer.\n",
    "\n",
    "In this scenario, the times will be exponentially distributed with rate parameter, $\\lambda$. The number of customers that arrive during opening hours will be Poisson distributed also with rate parameter, $\\lambda$.\n",
    "\n",
    "**Stats:**\n",
    "- The mean is $\\mathbb{E}[X] = \\frac{1}{\\lambda}$\n",
    "- The variance is $\\mbox{Var}[X] = \\frac{1}{\\lambda^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exponential distribution, one parameter\n",
    "# Note: scipy defines the distribution as exp(-(x-loc)/scale)/scale\n",
    "\n",
    "scale_slider = FloatSlider(value=0.5, min=0.5, max=2.0, step=0.05, description='$1/\\lambda$')\n",
    "interact(\n",
    "    interact_dist(sp.stats.expon, xlim=[0, 6], ylim=[0, 2]),\n",
    "    scale=scale_slider\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Poisson distribution as waiting times\n",
    "lam = 3.5\n",
    "T = 3.\n",
    "N = 1000\n",
    "M = 100\n",
    "\n",
    "exp_rvs = sp.stats.expon(scale=1./lam).rvs(size=[N, M])\n",
    "waiting_times_rvs = np.cumsum(exp_rvs, axis=1)\n",
    "poisson_rvs = (waiting_times_rvs <= T).sum(axis=1)\n",
    "plt.figure()\n",
    "plot_hist_dist_discrete(poisson_rvs, sp.stats.poisson(mu=lam * T), title=\"Poisson 1\")\n",
    "\n",
    "poisson_rvs = np.bincount(np.floor(np.cumsum(exp_rvs) / T).astype(int))\n",
    "plt.figure()\n",
    "plot_hist_dist_discrete(poisson_rvs, sp.stats.poisson(mu=lam * T), title=\"Poisson 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the PDF of the exponential distribution is similar to the PMF of the geometric distribution. The exponential distribution also has a memoryless property:\n",
    "\n",
    "$$ \\mathbb{P}[X=x+k \\mid X \\ge k]= \\lambda e^{-\\lambda x} = \\mathbb{P}[X=x]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The memoryless property of the exponential distribution\n",
    "\n",
    "lam=.2\n",
    "a = 3.2\n",
    "N = 2000\n",
    "\n",
    "dist = sp.stats.expon(scale=1./lam)\n",
    "rvs = dist.rvs(size=N)\n",
    "plot_hist_dist(rvs[rvs >= a] - a, dist, title=\"Conditional Exponential\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the most famous distribution in all of statistics is the normal (or Gaussian) distribution.  It is parameterized by its mean $\\mu$ and standard deviation $\\sigma$, and the distribution is denoted $N(\\mu, \\sigma^2)$.  Its PDF is given by\n",
    "\n",
    "$$ p(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(\\frac{(x - \\mu)^2}{2 \\sigma^2} \\right) \\,.$$\n",
    "\n",
    "**Stats:**\n",
    "- The mean is $\\mathbb{E}[X] = \\mu$\n",
    "- The variance is $\\mbox{Var}[X] = \\sigma^2$.\n",
    "\n",
    "**Examples:**\n",
    "- The height or weight of students in a classroom.\n",
    "- The distribution of test scores on the SAT.\n",
    "\n",
    "**Useful Facts:** Let's write $n(x  \\mid  \\mu, \\sigma^2)$ to be the normal PDF with mean $\\mu$ and standard deviation $\\sigma$.  A little bit of algebra shows that the distribution satisfies translational symmetry,\n",
    "\n",
    "$$ n(x  \\mid  \\mu, \\sigma^2) = n(x - \\mu  \\mid  0, \\sigma^2) $$\n",
    "\n",
    "and scaling symmetry\n",
    "\n",
    "$$ n(x  \\mid  \\mu, \\sigma^2) = n\\left(\\frac{x - \\mu}{\\sigma} \\,\\Big|\\, 0, 1\\right)\\,. $$\n",
    "\n",
    "A standard normal distribution is one with mean $\\mu = 0$ and standard deviation $\\sigma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normal distribution, two parameters\n",
    "\n",
    "mu_slider = FloatSlider(value=0, min=-3, max=3, step=0.05, description='$\\mu$')\n",
    "sigma_slider = FloatSlider(value=1.0, min=0.5, max=2.0, step=0.05, description='$\\sigma$')\n",
    "interact(\n",
    "    interact_dist(sp.stats.norm, xlim=[-6, 6], ylim=[0, 0.8]),\n",
    "    loc=mu_slider, scale=sigma_slider\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, when we look at random variables, we want to understand their mean.  It turns out this behaves in a vary predictable way.  As $n \\to \\infty$,\n",
    "\n",
    "$$ \\sum_{k=1}^n \\frac{X_k}{n} \\longrightarrow \\mathbb{E}[X] \\,.$$\n",
    "\n",
    "To be more precise, as $n \\to \\infty$, we have that the mean becomes a random variable that's distributed normally,\n",
    "$$ \\sum_{k=1}^n \\frac{X_k}{n} \\sim N\\left(\\mathbb{E}[X], \\frac{\\mbox{Var}[X]}{n} \\right) .$$\n",
    "\n",
    "Let's see that in action.  We'll plot very different distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A variety of distributions, when averaged, are normal\n",
    "\n",
    "dists = (\n",
    "    sp.stats.uniform(),\n",
    "    sp.stats.beta(a=2., b=4.),\n",
    "    sp.stats.expon(),\n",
    "    sp.stats.chi2(df=10),\n",
    "    sp.stats.gamma(a=1.99),\n",
    "    sp.stats.norm(),\n",
    ")\n",
    "\n",
    "N=2000\n",
    "M=100\n",
    "\n",
    "plt.figure(figsize=(8, 11))\n",
    "for k, dist in enumerate(dists):\n",
    "    ax = plt.subplot(6,2,2*k+1)\n",
    "    plot_hist_dist(dist.rvs(size=N), dist, title=dist.dist.name + ' (original)', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(6,2,2*k+2)\n",
    "    m, v = dist.stats()\n",
    "    rvs = dist.rvs(size=(N,M)).mean(axis=1)\n",
    "    plot_hist_dist(rvs, sp.stats.norm(loc=m, scale=np.sqrt(v / M)), \n",
    "                   title=dist.dist.name + ' (central limit)', ax=ax)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Beta distribution is a distribution on $[0,1]$ that is very closely rated to the binomial distribution.  It has a PDF given by\n",
    "\n",
    "$$ p(x) = \\frac{x^{\\alpha-1} (1-x)^{\\beta - 1}}{\\mbox{B}(\\alpha, \\beta)} $$\n",
    "\n",
    "where $\\mbox{B}(\\alpha, \\beta)$ is the beta function, which is just used as a normalizing constant to make $p(x)$ a probability distribution.  We'll denote this distribution $B(\\alpha, \\beta)$.\n",
    "\n",
    "**Examples:**\n",
    "- If $\\alpha = 1$ and $\\beta = 1$, this is just the uniform distribution.\n",
    "\n",
    "**Stats:**\n",
    "- The mean is $\\mathbb{E}[X] = \\frac{\\alpha}{\\alpha + \\beta}$\n",
    "- The variance is $\\mbox{Var}[X] = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Beta distribution, two parameters\n",
    "\n",
    "a_slider = IntSlider(value=1, min=1, max=5, description='a')\n",
    "b_slider = IntSlider(value=1, min=1, max=5, description='b')\n",
    "interact(\n",
    "    interact_dist(sp.stats.beta, xlim=[0, 1], ylim=[0, 3]),\n",
    "    a=a_slider, b=b_slider\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' Theorem is a simple rule about conditional probabilities that has profound consequences.\n",
    "\n",
    "$$ \\mathbb{P}[X=x \\mid Y=y] = \\frac{\\mathbb{P}[Y=y \\mid X=x] \\cdot \\mathbb{P}[X=x]}{\\mathbb{P}[Y=y]} $$\n",
    "\n",
    "Mathematically, it's simply derived from the fact that\n",
    "$$ \\mathbb{P}[X=x\\mid Y=y] \\cdot \\mathbb{P}[Y=y] = \\mathbb{P}[X=x, Y=y] = \\mathbb{P}[Y=y, X=x] = \\mathbb{P}[Y=y \\mid X=x] \\cdot \\mathbb{P}[X=x] \\,.$$\n",
    "\n",
    "While it seems trivial, it tells us what we know about one variable ($X$) given information about another ($Y$). \n",
    "\n",
    "**Example:** Assume we have  biased Bernoulli $X$ that takes on $1$ with probability $P$.  We don't know $P$; we can call that a random variable as well, and let's assume it's beta distributed with parameters $\\alpha$ and $\\beta$ (denoted $B(\\alpha, \\beta)$).  By observing flips of $X$, we can intuitively infer the value of $P$.\n",
    "\n",
    "Using Bayes' Theorem, we have that the new distribution of\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mathbb{P}[P=p \\mid X=1] \\propto\\,\\, & \\mathbb{P}[X=1 \\mid P=p] \\cdot \\mathbb{P}[P=p] \\\\\n",
    " \\propto\\,\\, & p \\cdot p^{\\alpha-1} (1-p)^{\\beta-1} \\\\\n",
    " \\propto\\,\\, & p^{\\alpha} (1-p)^{\\beta-1}\n",
    "\\end{align}\n",
    "$$\n",
    "It's not hard to show that\n",
    "$$ \\mathbb{P}[P=p \\mid X=1] = B(\\alpha+1, \\beta)\\,. $$\n",
    "Similarly, \n",
    "$$ \\mathbb{P}[P=p \\mid X=0] = B(\\alpha, \\beta+1)\\,. $$\n",
    "\n",
    "Stated differently, if we have $P$ distributed as $B(\\alpha, \\beta)$ (the **prior**), and if we observe $X=1$, we update our estimate of the distribution of $P$ (the **posterior**) to $B(\\alpha+1, \\beta)$, and if we observe $X=0$, we update the posterior to $B(\\alpha, \\beta+1)$.  The fact that the Bayes' Theorem equations can be solved in closed form makes the Bernoulli and Beta distributions **Conjugate Priors**.\n",
    "\n",
    "**Example:** We serve ads to individuals, and our prior belief that they will click on an ad is $P$ distributed $B(\\alpha, \\beta)$.  If we see them click on the ad, then our posterior for $P$ is $B(\\alpha+1, \\beta)$, and if they do not, then our posterior is $B(\\alpha, \\beta+1)$.\n",
    "\n",
    "Continuing on, if we show another ad to the same individual, we can get this process to update again.  So if they click on 2 of the 10 ads we show them, then our posterior $P$ is $B(\\alpha+2, \\beta+8)$.\n",
    "\n",
    "In fact, the canonical interpretation of $B(\\alpha, \\beta)$ is our belief about the odds of some $X$ being successful if we have seen $\\alpha$ 1s and $\\beta$ 0s.\n",
    "\n",
    "Below is Bayes' Theorem in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Beta and binomial are conjugate priors\n",
    "\n",
    "def binomial_bayes(a_0, b_0, a_1, b_1):\n",
    "    prior = sp.stats.beta(a=a_0, b=b_0)\n",
    "    posterior = sp.stats.beta(a=a_0+a_1, b=b_0+b_1)\n",
    "\n",
    "    prior_rvs = prior.rvs(size=10000)\n",
    "    variates = sp.stats.binom(n=a_1+b_1, p=prior_rvs).rvs()\n",
    "    posterior_rvs = prior_rvs[variates == a_1]\n",
    "\n",
    "    ax1 = plt.subplot(2,1,1)\n",
    "    plot_hist_dist(prior_rvs, prior, title=\"Prior Beta(a={a}, b={b})\".format(**prior.kwds), ax=ax1)\n",
    "    ax2 = plt.subplot(2,1,2, sharex=ax1)\n",
    "    plot_hist_dist(posterior_rvs, posterior, title=\"Posterior Beta(a={a}, b={b})\".format(**posterior.kwds), ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "binomial_bayes(a_0=2, b_0=2, a_1=6, b_1=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with the previous example,  we expect these three facts to be true intuitively:\n",
    "- If $X=1$, we expect our estimate of $P$ to go up.\n",
    "- If $X=0$, we expect our estimate of $P$ to go down.\n",
    "- In either case, we expect that we have learned more information about $P$ and we expect the uncertainty of $P$ to decrease.\n",
    "\n",
    "We can see these 3 facts in the plots of $\\beta$ below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Intuition of beta distribution in terms of the Bernoulli\n",
    "\n",
    "dists = (\n",
    "    sp.stats.beta(a=2, b=2),\n",
    "    sp.stats.beta(a=2, b=6),\n",
    "    sp.stats.beta(a=6, b=2),\n",
    "    sp.stats.beta(a=6, b=6),\n",
    ")\n",
    "\n",
    "N=2000\n",
    "M=100\n",
    "\n",
    "plt.figure()\n",
    "for k, dist in enumerate(dists):\n",
    "    ax = plt.subplot(2,2,k+1)\n",
    "    plot_hist_dist(\n",
    "        dist.rvs(size=N),\n",
    "        dist,\n",
    "        title=\"B(a={a},b={b})\".format(**dist.kwds),\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlim([0, 1])\n",
    "plt.suptitle('Distributions')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 [Pragmatic Institute](https://www.pragmaticmarketing.com/data-science). This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
